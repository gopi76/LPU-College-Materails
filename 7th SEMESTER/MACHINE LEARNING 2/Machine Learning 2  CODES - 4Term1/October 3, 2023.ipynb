{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45639ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b96491",
   "metadata": {},
   "source": [
    "# Deterministic Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bb4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_policy(state):\n",
    "    if state == \"S1\":\n",
    "        return \"A1\"\n",
    "    elif state == \"S2\":\n",
    "        return \"A2\"\n",
    "    elif state == \"S3\":\n",
    "        return \"A1\"\n",
    "    else:\n",
    "        return \"Invalid state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149f29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_environment(state, action):\n",
    "    \n",
    "    transitions = {\n",
    "        \"S1\" : {\"A1\" : \"S2\", \"A2\" : \"S3\"},\n",
    "        \"S2\" : {\"A1\" : \"S1\", \"A2\" : \"S3\"},\n",
    "        \"S3\" : {\"A1\" : \"S1\", \"A2\" : \"S2\"}\n",
    "    }\n",
    "    \n",
    "    rewards = {\n",
    "        \"S1\" : {\"A1\" : {\"S2\" : 10}, \"A2\" : {\"S3\" : 0 }},\n",
    "        \"S2\" : {\"A1\" : {\"S1\" : 0},  \"A2\" : {\"S3\" : 5 }},\n",
    "        \"S3\" : {\"A1\" : {\"S1\" : -1}, \"A2\" : {\"S2\" : 0 }}\n",
    "    }\n",
    "    \n",
    "    next_state = transitions[state][action]\n",
    "    reward = rewards[state][action][next_state]\n",
    "    \n",
    "    return [next_state, reward] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68360bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state      = \"S1\"\n",
    "cummulative_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "941b3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State : S1, Action : A1\n",
      "Next state : S2. Reward : 10 \n",
      "\n",
      "Current State : S2, Action : A2\n",
      "Next state : S3. Reward : 5 \n",
      "\n",
      "Current State : S3, Action : A1\n",
      "Next state : S1. Reward : -1 \n",
      "\n",
      "Current State : S1, Action : A1\n",
      "Next state : S2. Reward : 10 \n",
      "\n",
      "Current State : S2, Action : A2\n",
      "Next state : S3. Reward : 5 \n",
      "\n",
      "Current State : S3, Action : A1\n",
      "Next state : S1. Reward : -1 \n",
      "\n",
      "Current State : S1, Action : A1\n",
      "Next state : S2. Reward : 10 \n",
      "\n",
      "Current State : S2, Action : A2\n",
      "Next state : S3. Reward : 5 \n",
      "\n",
      "Current State : S3, Action : A1\n",
      "Next state : S1. Reward : -1 \n",
      "\n",
      "Current State : S1, Action : A1\n",
      "Next state : S2. Reward : 10 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    action = deterministic_policy(current_state)\n",
    "    print(f\"Current State : {current_state}, Action : {action}\")\n",
    "        \n",
    "    next_state, reward = simulate_environment(current_state, action)\n",
    "    print(f\"Next state : {next_state}. Reward : {reward} \\n\")\n",
    "        \n",
    "    cummulative_reward += reward\n",
    "    current_state  =  next_state\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b741c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cummulative Reward :  52\n"
     ]
    }
   ],
   "source": [
    "print(\"Cummulative Reward : \", cummulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03e856",
   "metadata": {},
   "source": [
    "# Stochastic Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c085455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_policy(state):\n",
    "    if state == \"S1\":\n",
    "        action_probs = { \"A1\" : 0.6, \"A2\" : 0.4}\n",
    "    elif state == \"S2\":\n",
    "        action_probs = { \"A1\" : 0.3, \"A2\" : 0.7}\n",
    "    elif state == \"S3\":\n",
    "        action_probs = { \"A1\" : 0.8, \"A2\" : 0.2}\n",
    "    else:\n",
    "        return ValueError(\"Invalid State\")\n",
    "    \n",
    "    action = np.random.choice(list(action_probs.keys()), p = list(action_probs.values()))\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f2982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_environment(state, action):\n",
    "    \n",
    "    transitions = {\n",
    "        \"S1\" : {\"A1\" : \"S2\", \"A2\" : \"S3\"},\n",
    "        \"S2\" : {\"A1\" : \"S1\", \"A2\" : \"S3\"},\n",
    "        \"S3\" : {\"A1\" : \"S1\", \"A2\" : \"S2\"}\n",
    "    }\n",
    "    \n",
    "    rewards = {\n",
    "        \"S1\" : {\"A1\" : {\"S2\" : 10}, \"A2\" : {\"S3\" : 0 }},\n",
    "        \"S2\" : {\"A1\" : {\"S1\" : 0},  \"A2\" : {\"S3\" : 5 }},\n",
    "        \"S3\" : {\"A1\" : {\"S1\" : -1}, \"A2\" : {\"S2\" : 0 }}\n",
    "    }\n",
    "    \n",
    "    next_state = transitions[state][action]\n",
    "    reward = rewards[state][action][next_state]\n",
    "    \n",
    "    return [next_state, reward] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f748ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state      = \"S1\"\n",
    "cummulative_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ce35e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State : S1, Action : A1\n",
      "Next state : S2. Reward : 10 \n",
      "\n",
      "Current State : S2, Action : A1\n",
      "Next state : S1. Reward : 0 \n",
      "\n",
      "Current State : S1, Action : A1\n",
      "Next state : S2. Reward : 10 \n",
      "\n",
      "Current State : S2, Action : A2\n",
      "Next state : S3. Reward : 5 \n",
      "\n",
      "Current State : S3, Action : A1\n",
      "Next state : S1. Reward : -1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    action = stochastic_policy(current_state)\n",
    "    print(f\"Current State : {current_state}, Action : {action}\")\n",
    "        \n",
    "    next_state, reward = simulate_environment(current_state, action)\n",
    "    print(f\"Next state : {next_state}. Reward : {reward} \\n\")\n",
    "        \n",
    "    cummulative_reward += reward\n",
    "    current_state  =  next_state\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c3d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cummulative Reward :  24\n"
     ]
    }
   ],
   "source": [
    "print(\"Cummulative Reward : \", cummulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769768c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
